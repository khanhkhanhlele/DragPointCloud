{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import gc\n",
    "import psutil\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import torch.distributed as dist\n",
    "from torch import optim\n",
    "from trainers.base_trainer import BaseTrainer\n",
    "from utils.ema import EMA\n",
    "from utils.model_helper import import_model, loss_fn\n",
    "from utils.vis_helper import visualize_point_clouds_3d\n",
    "from utils.eval_helper import compute_NLL_metric \n",
    "from utils import model_helper, exp_helper, data_helper\n",
    "from utils.data_helper import normalize_point_clouds\n",
    "from utils.diffusion_pvd import DiffusionDiscretized\n",
    "from utils.diffusion_continuous import make_diffusion, DiffusionBase\n",
    "from utils.checker import *\n",
    "from utils import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import third_party.pvcnn.functional as pvcnn_fn\n",
    "from timeit import default_timer as timer\n",
    "from torch.optim import Adam as FusedAdam\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from trainers import common_fun_prior_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1       , 0.3010101 , 0.5020202 , 0.7030303 , 0.9040404 ,\n",
       "       1.10505051, 1.30606061, 1.50707071, 1.70808081, 1.90909091])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.diffusion import make_beta_schedule\n",
    "beta_start = 0.1\n",
    "beta_end = 20.0\n",
    "num_steps = 100\n",
    "mode = 'linear'\n",
    "\n",
    "betas = make_beta_schedule(\n",
    "            mode, beta_start, beta_end, num_steps).numpy()\n",
    "betas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_base_constants(betas, diffusion_steps):\n",
    "    \"\"\"\n",
    "    Generates torch tensors with basic constants for all timesteps.\n",
    "    \"\"\"\n",
    "    betas_np = betas  # self._generate_betas_from_continuous_fun(diffusion_steps)\n",
    "\n",
    "    alphas_np = 1.0 - betas_np\n",
    "    alphas_cumprod = alpha_bars_np = np.cumprod(alphas_np)\n",
    "    snr = 1.0 / (1 - alphas_cumprod) - 1\n",
    "\n",
    "    # posterior variances only make sense for t>1, hence the array is short by 1\n",
    "    betas_post_np = betas_np[1:] * \\\n",
    "        (1.0 - alpha_bars_np[:-1]) / (1.0 - alpha_bars_np[1:])\n",
    "    # we add beta_post_2 to the beginning of both beta arrays, since this is used as final decoder variance and\n",
    "    # requires special treatment (as in diffusion paper)\n",
    "    betas_post_init_np = np.append(betas_post_np[0], betas_post_np)\n",
    "    #betas_init_np = np.append(betas_post_np[0], betas_np[1:])\n",
    "\n",
    "    betas_init = torch.from_numpy(betas_np).float().cuda()\n",
    "    snr = torch.from_numpy(snr).float().cuda()\n",
    "    alphas = torch.from_numpy(alphas_np).float().cuda()\n",
    "    alpha_bars = torch.from_numpy(alpha_bars_np).float().cuda()\n",
    "    betas_post_init = torch.from_numpy(betas_post_init_np).float().cuda()\n",
    "\n",
    "    return betas_init, alphas, alpha_bars, betas_post_init, snr\n",
    "\n",
    "betas_init, alphas, alpha_bars, betas_post_init, snr = _generate_base_constants(betas, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(type(betas_init))\n",
    "# print(alphas)\n",
    "# print(alpha_bars)\n",
    "print(betas_post_init.shape)\n",
    "# print(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n",
      "tensor([0.2500])\n",
      "tensor([-0.6931])\n",
      "tensor([0.0500, 0.1500, 0.2500, 0.3500, 0.4500])\n"
     ]
    }
   ],
   "source": [
    "def get_p_log_scales(betas_init, betas_post_init, timestep, stddev_type):\n",
    "    \"\"\"\n",
    "    Grab log std devs. of backward denoising process p, if we decide to fix them.\n",
    "    \"\"\"\n",
    "    if stddev_type == 'beta':\n",
    "        # use diffusion variances, except for t=1, for which we use posterior variance beta_post_2\n",
    "        index = torch.tensor([timestep - 1], dtype=torch.long)  # Tạo tensor chỉ số\n",
    "        return 0.5 * torch.log(torch.gather(betas_init, 0, index))\n",
    "    elif stddev_type == 'beta_post':\n",
    "        # use diffusion posterior variances, except for t=1, for which there is no posterior, so we use beta_post_2\n",
    "        index = torch.tensor([timestep - 1], dtype=torch.long)  # Tạo tensor chỉ số\n",
    "        print(index)\n",
    "        temp = torch.gather(betas_post_init, 0, index)\n",
    "        print(temp)\n",
    "        return 0.5 * torch.log(torch.gather(betas_post_init, 0, index))\n",
    "    elif stddev_type == 'learn':\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError('Unknown stddev_type: {}'.format(stddev_type))\n",
    "\n",
    "# Ví dụ kiểm tra\n",
    "betas_init = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])  # Mô phỏng một tensor beta\n",
    "betas_post_init = torch.tensor([0.05, 0.15, 0.25, 0.35, 0.45])  # Mô phỏng một tensor beta_post\n",
    "\n",
    "# Gọi hàm\n",
    "result = get_p_log_scales(betas_init, betas_post_init, 3, 'beta_post')\n",
    "print(result)\n",
    "print(betas_post_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5477, 1.0954, 1.6432, 2.1909]]],\n",
      "\n",
      "\n",
      "        [[[0.5477, 1.0954, 1.6432, 2.1909]]],\n",
      "\n",
      "\n",
      "        [[[0.5477, 1.0954, 1.6432, 2.1909]]],\n",
      "\n",
      "\n",
      "        [[[0.5477, 1.0954, 1.6432, 2.1909]]]])\n"
     ]
    }
   ],
   "source": [
    "def get_mixing_component(x_noisy, time_step, enabled, _alpha_bars):\n",
    "    size = x_noisy.size()\n",
    "    alpha_bars = torch.gather(_alpha_bars, 0, time_step-1)\n",
    "    one_minus_alpha_bars = utils.view4D(torch.sqrt(1.0 - alpha_bars), size)\n",
    "    return x_noisy * one_minus_alpha_bars\n",
    "\n",
    "alpha_bars = torch.tensor([0.9, 0.8, 0.7, 0.6, 0.5])\n",
    "get_mixing_component(torch.tensor([1, 2, 3, 4]), 3, True, alpha_bars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_ddim(self, model, num_samples, shape, temp=1.0, enable_autocast=False, \n",
    "            is_image=True, prior_var=1.0, conditional_input=None, ddim_step=100, skip_type='uniform', kappa=1.0,\n",
    "            clip_feat=None, grid_emb=None, x_noisy=None, dae_index=-1):\n",
    "    model.eval()\n",
    "    x_noisy_size = [num_samples] + shape\n",
    "    x_noisy = torch.randn(\n",
    "        size=x_noisy_size, device='cuda') if x_noisy is None else x_noisy.cuda()\n",
    "    output_list = []\n",
    "    S = ddim_step\n",
    "    if skip_type == 'uniform':\n",
    "        c = (self._diffusion_steps - 1.0) / (S - 1.0)\n",
    "        list_tau = [np.floor(i * c) for i in range(S)]\n",
    "        list_tau = [int(s) for s in list_tau]\n",
    "    user_defined_steps = sorted(list(list_tau), reverse=True)\n",
    "    T_user = len(user_defined_steps)\n",
    "\n",
    "    Alpha_bar = self._alpha_bars\n",
    "    for i, t in enumerate(user_defined_steps):\n",
    "        tau = t \n",
    "        timestep = torch.ones(\n",
    "            num_samples, dtype=torch.int64, device='cuda') * (t+1)\n",
    "        fixed_log_scales = self.get_p_log_scales(\n",
    "                timestep=timestep, stddev_type=self._denoising_stddevs)\n",
    "        mixing_component = self.get_mixing_component(\n",
    "                x_noisy, timestep, enabled=model.mixed_prediction)\n",
    "        with autocast(enable_autocast):\n",
    "            pred_logits = model(\n",
    "                    x=x_noisy, t=timestep.float(), condition_input=condition_input)\n",
    "            logits = utils.get_mixed_prediction(\n",
    "                    model.mixed_prediction, pred_logits, model.mixing_logit, mixing_component)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
      "[908, 918, 928, 938, 948, 958, 968, 978, 988, 999]\n",
      "[999, 988, 978, 968, 958, 948, 938, 928, 918, 908, 898, 888, 877, 867, 857, 847, 837, 827, 817, 807, 797, 787, 777, 766, 756, 746, 736, 726, 716, 706, 696, 686, 676, 666, 655, 645, 635, 625, 615, 605, 595, 585, 575, 565, 555, 544, 534, 524, 514, 504, 494, 484, 474, 464, 454, 444, 433, 423, 413, 403, 393, 383, 373, 363, 353, 343, 333, 322, 312, 302, 292, 282, 272, 262, 252, 242, 232, 222, 211, 201, 191, 181, 171, 161, 151, 141, 131, 121, 111, 100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 0]\n"
     ]
    }
   ],
   "source": [
    "diffusion_steps = 1000\n",
    "S = 100\n",
    "num_samples = 2\n",
    "\n",
    "c = (diffusion_steps - 1.0) / (S - 1.0)\n",
    "list_tau = [np.floor(i * c) for i in range(S)]\n",
    "list_tau = [int(s) for s in list_tau]\n",
    "user_defined_steps = sorted(list(list_tau), reverse=True)\n",
    "print(list_tau[:10])\n",
    "print(list_tau[-10:])\n",
    "print(user_defined_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1000, 1000])\n",
      "tensor([989, 989])\n",
      "tensor([979, 979])\n",
      "tensor([969, 969])\n",
      "tensor([959, 959])\n",
      "tensor([949, 949])\n",
      "tensor([939, 939])\n",
      "tensor([929, 929])\n",
      "tensor([919, 919])\n",
      "tensor([909, 909])\n",
      "tensor([899, 899])\n",
      "tensor([889, 889])\n",
      "tensor([878, 878])\n",
      "tensor([868, 868])\n",
      "tensor([858, 858])\n",
      "tensor([848, 848])\n",
      "tensor([838, 838])\n",
      "tensor([828, 828])\n",
      "tensor([818, 818])\n",
      "tensor([808, 808])\n",
      "tensor([798, 798])\n",
      "tensor([788, 788])\n",
      "tensor([778, 778])\n",
      "tensor([767, 767])\n",
      "tensor([757, 757])\n",
      "tensor([747, 747])\n",
      "tensor([737, 737])\n",
      "tensor([727, 727])\n",
      "tensor([717, 717])\n",
      "tensor([707, 707])\n",
      "tensor([697, 697])\n",
      "tensor([687, 687])\n",
      "tensor([677, 677])\n",
      "tensor([667, 667])\n",
      "tensor([656, 656])\n",
      "tensor([646, 646])\n",
      "tensor([636, 636])\n",
      "tensor([626, 626])\n",
      "tensor([616, 616])\n",
      "tensor([606, 606])\n",
      "tensor([596, 596])\n",
      "tensor([586, 586])\n",
      "tensor([576, 576])\n",
      "tensor([566, 566])\n",
      "tensor([556, 556])\n",
      "tensor([545, 545])\n",
      "tensor([535, 535])\n",
      "tensor([525, 525])\n",
      "tensor([515, 515])\n",
      "tensor([505, 505])\n",
      "tensor([495, 495])\n",
      "tensor([485, 485])\n",
      "tensor([475, 475])\n",
      "tensor([465, 465])\n",
      "tensor([455, 455])\n",
      "tensor([445, 445])\n",
      "tensor([434, 434])\n",
      "tensor([424, 424])\n",
      "tensor([414, 414])\n",
      "tensor([404, 404])\n",
      "tensor([394, 394])\n",
      "tensor([384, 384])\n",
      "tensor([374, 374])\n",
      "tensor([364, 364])\n",
      "tensor([354, 354])\n",
      "tensor([344, 344])\n",
      "tensor([334, 334])\n",
      "tensor([323, 323])\n",
      "tensor([313, 313])\n",
      "tensor([303, 303])\n",
      "tensor([293, 293])\n",
      "tensor([283, 283])\n",
      "tensor([273, 273])\n",
      "tensor([263, 263])\n",
      "tensor([253, 253])\n",
      "tensor([243, 243])\n",
      "tensor([233, 233])\n",
      "tensor([223, 223])\n",
      "tensor([212, 212])\n",
      "tensor([202, 202])\n",
      "tensor([192, 192])\n",
      "tensor([182, 182])\n",
      "tensor([172, 172])\n",
      "tensor([162, 162])\n",
      "tensor([152, 152])\n",
      "tensor([142, 142])\n",
      "tensor([132, 132])\n",
      "tensor([122, 122])\n",
      "tensor([112, 112])\n",
      "tensor([101, 101])\n",
      "tensor([91, 91])\n",
      "tensor([81, 81])\n",
      "tensor([71, 71])\n",
      "tensor([61, 61])\n",
      "tensor([51, 51])\n",
      "tensor([41, 41])\n",
      "tensor([31, 31])\n",
      "tensor([21, 21])\n",
      "tensor([11, 11])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(user_defined_steps):\n",
    "    tau = t \n",
    "    timestep = torch.ones(\n",
    "        num_samples, dtype=torch.int64) * (t+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "num_samples = 2 \n",
    "shape = [2,1]\n",
    "size = [num_samples] + shape\n",
    "x_noisy = torch.randn(size=size)\n",
    "print(x_noisy.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples_vada(shape, dae, diffusion, vae, num_samples, \n",
    "                            enable_autocast, ode_eps=0.00001, ode_solver_tol=1e-5,\n",
    "                            ode_sample=False, prior_var-1.0, temp=1.0, vae_temp=1.0,\n",
    "                            noise=None, need_denoise=False, ddim_step=0, clip_feat=None):\n",
    "    output = {}\n",
    "    if not ode_sample:\n",
    "        assert isinstance(diffusion, DiffusionDiscretized), 'Regular sampling requires disc. diffusion!'\n",
    "        assert noise is None, 'Noise is not used in ancestral sampling.'\n",
    "        total_step = diffusion._diffusion_steps\n",
    "        if ddim_step > 0:\n",
    "            eps, eps_list = diffusion.run_ddim(dae, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Model.__init__() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_lib\u001b[38;5;241m.\u001b[39mModel()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m----> 7\u001b[0m vae \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels.vae_adain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(model_name):\n\u001b[1;32m      3\u001b[0m     model_lib \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(model_name)\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mTypeError\u001b[0m: Model.__init__() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "def build_model(model_name):\n",
    "    model_lib = importlib.import_module(model_name)\n",
    "    model = model_lib.Model()\n",
    "    return model\n",
    "\n",
    "vae = build_model('models.vae_adain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DAE = import_model('models.latent_points_ada_localprior.PVCNN2Prior')\n",
    "model = \n",
    "class Trainer(BaseTrainer):\n",
    "    is_diffusion = 0\n",
    "\n",
    "    def train_iter(self, data, *args, **kwargs):\n",
    "        device = 'cuda'\n",
    "        input_dim = 3 \n",
    "        loss_type = 'mse'\n",
    "        dae = DAE(args, num_input_channels, self.cfg).to(device)\n",
    "        vae = build_model('models.vae_adain').to(device)\n",
    "        diffusion = DiffusionDiscretized()\n",
    "        dae_optimizer = optim.Adam(param_dict_dae,\n",
    "                                   lr=args.learning_rate_dae,\n",
    "                                   betas=(cfgopt.beta1, cfgopt.beta2),\n",
    "                                   weight_decay=cfgopt.weight_decay)\n",
    "        vae_optimizer = optim.Adam(vae.parameters(),\n",
    "                                   lr=args.learning_rate_min_vae,\n",
    "                                   betas=(cfgopt.beta1, cfgopt.beta2),\n",
    "                                   weight_decay=cfgopt.weight_decay)\n",
    "        args = self.cfg.sde\n",
    "        num_total_iter = cfg.trainer.epochs * len(train_loader)\n",
    "        dae_sn_calculator = self.dae_sn_calculator\n",
    "        vae_sn_calculator = self.vae_sn_calculator\n",
    "        grad_scalar = self.grad_scalar\n",
    "\n",
    "        global_step = step = kwargs.get('step', None)\n",
    "        no_update = kwargs.get('no_update', False)\n",
    "\n",
    "        # update_lr\n",
    "        warmup_iters = len(self.train_loader) * args.warmup_epochs\n",
    "        utils.update_lr(args, global_step, warmup_iters,\n",
    "                        dae_optimizer, vae_optimizer)\n",
    "\n",
    "        # input\n",
    "        tr_pts = data['tr_points'].to(device)  # (B, Npoints, 3)\n",
    "        # the noisy points, used in trainers/voxel2pts.py and trainers/voxel2pts_ada.py\n",
    "        inputs = data['input_pts'].to(device) if 'input_pts' in data else None\n",
    "        B = batch_size = tr_pts.size(0)\n",
    "\n",
    "        # optimize vae params\n",
    "        vae_optimizer.zero_grad()\n",
    "        output = self.compute_loss_vae(tr_pts, global_step, inputs=inputs)\n",
    "\n",
    "        if args.train_dae:\n",
    "            eps = output['eps'].detach() #B, D, N, 1\n",
    "            dae_optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                noise_p = torch.rand(size=eps.size(), device=device)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0743, 0.5052, 0.4140]])\n",
      "55\n",
      "50\n",
      "61\n",
      "7\n",
      "6\n",
      "Updated handle points: [[57, 56], [34, 33]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hàm point_tracking như đã định nghĩa\n",
    "def point_tracking(F0, F1, handle_points, handle_points_init, args):\n",
    "    with torch.no_grad():\n",
    "        _, _, max_r, max_c = F0.shape\n",
    "        for i in range(len(handle_points)):\n",
    "            pi0, pi = handle_points_init[i], handle_points[i]\n",
    "            f0 = F0[:, :, int(pi0[0]), int(pi0[1])]\n",
    "\n",
    "            \n",
    "\n",
    "            r1, r2 = max(0, int(pi[0]) - args.r_p), min(max_r, int(pi[0]) + args.r_p + 1)\n",
    "            c1, c2 = max(0, int(pi[1]) - args.r_p), min(max_c, int(pi[1]) + args.r_p + 1)\n",
    "            \n",
    "            \n",
    "            F1_neighbor = F1[:, :, r1:r2, c1:c2]\n",
    "            all_dist = (f0.unsqueeze(dim=-1).unsqueeze(dim=-1) - F1_neighbor).abs().sum(dim=1)\n",
    "            all_dist = all_dist.squeeze(dim=0)\n",
    "            row, col = divmod(all_dist.argmin().item(), all_dist.shape[-1])\n",
    "            if i == 0:\n",
    "                print(f0)\n",
    "                print(pi[1])\n",
    "                print(c1)\n",
    "                print(c2)\n",
    "                print(row)\n",
    "                print(col)\n",
    "            \n",
    "            handle_points[i][0] = r1 + row\n",
    "            handle_points[i][1] = c1 + col\n",
    "        return handle_points\n",
    "\n",
    "# Định nghĩa class args với tham số r_p (bán kính tìm kiếm)\n",
    "class Args:\n",
    "    def __init__(self, r_p):\n",
    "        self.r_p = r_p\n",
    "\n",
    "# Giả lập dữ liệu đầu vào\n",
    "B, C, H, W = 1, 3, 100, 100  # batch size, số channels, chiều cao và chiều rộng của feature maps\n",
    "\n",
    "# Feature maps F0 và F1 ngẫu nhiên\n",
    "F0 = torch.rand(B, C, H, W)\n",
    "F1 = torch.rand(B, C, H, W)\n",
    "\n",
    "# Danh sách các điểm điều khiển ban đầu và hiện tại (giả lập)\n",
    "handle_points_init = [[50, 50], [30, 30]]  # ví dụ các điểm ở tọa độ (50, 50) và (30, 30)\n",
    "handle_points = [[55, 55], [35, 35]]       # ví dụ vị trí ban đầu tại (55, 55) và (35, 35)\n",
    "\n",
    "# Tạo đối tượng args với bán kính tìm kiếm r_p\n",
    "args = Args(r_p=5)\n",
    "\n",
    "# Gọi hàm point_tracking\n",
    "updated_points = point_tracking(F0, F1, handle_points, handle_points_init, args)\n",
    "\n",
    "# In ra kết quả\n",
    "print(\"Updated handle points:\", updated_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "temp = torch.tensor([0.0000, 1.7900, 0.0000])\n",
    "temp.repeat(2048, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
